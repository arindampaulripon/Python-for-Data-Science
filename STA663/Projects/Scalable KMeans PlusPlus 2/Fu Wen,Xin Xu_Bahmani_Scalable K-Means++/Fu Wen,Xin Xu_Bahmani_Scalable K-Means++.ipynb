{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> STA663 Final Project: Scalable K-means\n",
    "### <center> Xin Xu, Fu Wen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to its simplicity, the `k-means` algorithm is one of the most famous machine learning algorithms used to cluster data[1]. The main critical problem of this algorithm is that it might be blocked locally based on the initial random chosen centers. The `k-means++` algorithm is developed to solve this problem, spreading out the initial centers with an updating non-uniform distribution. However, `K-means++` has a limited applicability to large data sets due to its inherent sequential nature, which requires k passes through the whole data set to find the optimal initialization of centers. The `K-means||` algorithm in the paper \"Scalable K-Means++\" is the parallel version of the `k-means++`[2] and an improvement. Instead of sampling a single point, it oversamples a couple of centers in each iteration and guarantees the performance at the same time. \n",
    "\n",
    "In this report, we firstly implement the `K-means||` algorithm in Python. Then, we speed up the algorithm using `Cython` and parallelize the algorithm using `multiprocessing`. In the applicaiton part, we apply it to the `GAUSSMIXTURE` dataset simulated as in [1] and the `SPAM` dataset from UC Irvine Machine Learning repository [3]. We compare the misclassification rate (for `GAUSSMIXTURE` dataset), clustering cost and runtime of the `k-means||` algorithm with the result of random initialization and the `k-means++`. In the end, we inplement `k-means||` algorithm in Spark using `pyspark.millib`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Background and Related Algorithm\n",
    "\n",
    "As one of the most popular clustering algorithms, the **k-means** algorithm has been widely used for the last half of the century[4]. The main idea is to randomly choose k centers, repeatedly assign each point to its nearest center and calculate the new centers by minimizing the sum of the squares of the distance in its neighborhood.\n",
    "\n",
    "The k-means algorithm has a critical problem of unreliable initialization[2]. This algorithm with an incorrect initialization cannot find a globally optimal solution but rests on a locally optimal solution. In addition, the running time before convergence is long. **K-means++** algorithm avoids this problem by finding k better initial centers. It first samples one random point uniformly from the data, then sets the subsequent k-1 centers with probability proportional to its contribution to the overall error given the previous centers. In contrast to the k-means algorithm, k-means++ reduces the probability of picking several initial centers in one cluster. However, the sequence initialization process also limits its applicability to large data sets or data with large k since the whole algorithm is not scalable.\n",
    "\n",
    "Bahmani et al. constructed the scalable k-means++ algorithm (**k-means||** algorithm) in their paper \"Scalable K-Means++\"[1]. The main idea is to sample more than one point (O(k)) in each round and repeat the process for fewer iterations ($O(logn)$). Then, the algorithm reclusters the $O(klogn)$ points generated from the above process into $k$ initial centers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Notation and the Algorithm\n",
    "\n",
    "Suppose $X = \\{x_1, \\dots, x_n \\}$ are d-dimentional points to be clustered and k is the number of cluster (a positer integer). \n",
    "\n",
    "For a subset $Y \\in X$, define the distance from a point $x$ to $Y$ as $d(x,Y) = min_{y \\in Y} \\| x-y\\|$, where $\\|x -y \\|$denote the Euclidean distance between $x$ and $y$, define the centroid of $Y$ as\n",
    "$$Centroid(Y) = \\frac{1}{|Y|} \\sum_{y \\in Y} y$$\n",
    "\n",
    "For a set of cluster centers $C = \\{ c_1, c_2 ,\\dots , c_k\\}$, define the _cost of $Y$_ with respect to $C$ as:\n",
    "\n",
    "$$ \\phi_Y(C) = \\sum_{y \\in Y} d(y,C)^2$$\n",
    "\n",
    "In `k-means||` algorithm, it set an oversampling factor $l = \\Omega (k)$. $l>1$ is an integer.\n",
    "\n",
    "\n",
    "Steps of `k-means||` algorithm:\n",
    "\n",
    "- Sample a point uniformaly from X as the first center $C$\n",
    "- Compute the cost of clustering based on this choice $\\phi_{X}(C)=\\psi$\n",
    "- for $O(log\\psi)$ times repeat:  \n",
    " - Independently sample $l$ points with probability $p_{x}=\\frac{l\\cdot d^{2}(x,C)}{\\phi_{X}(C)}$ as $C'$\n",
    " - $C=C\\cup C'$\n",
    "- For each point $x \\in C$, compute $w_{x}$ as the number of points in $X$ closer to x than other point in $C$\n",
    "- Get k clusters from reclustering those weighted points in $C$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-Means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distance_func.py\n"
     ]
    }
   ],
   "source": [
    "%%file distance_func.py\n",
    "import numpy as np\n",
    "\n",
    "def distance(data, centroids):\n",
    "    \"\"\" Calculate the distance from each data point to each center\n",
    "    Parameters:\n",
    "       data   n*d\n",
    "       center k*d\n",
    "    \n",
    "    Returns:\n",
    "       distence n*k \n",
    "    \"\"\"\n",
    "    ## calculate distence between each point to the centroids\n",
    "    dist = np.sum((data[:, np.newaxis, :] - centroids)**2, axis=2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kmeans_func.py\n"
     ]
    }
   ],
   "source": [
    "%%file kmeans_func.py\n",
    "import numpy as np\n",
    "from distance_func import distance\n",
    "\n",
    "def KMeans(data, k, centroids, max_iter = 10000): \n",
    "    \n",
    "    \"\"\" Apply the KMeans clustering algorithm\n",
    "    \n",
    "    Parameters:\n",
    "      data                        ndarrays data \n",
    "      k                           number of cluster\n",
    "      centroids                   initial centroids\n",
    "    \n",
    "    Returns:\n",
    "      \"Iteration before Coverge\"  time used to converge\n",
    "      \"Centroids\"                 the final centroids finded by KMeans    \n",
    "      \"Labels\"                    the cluster of each data   \n",
    "    \"\"\"\n",
    "    \n",
    "    n = data.shape[0] \n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iter:        \n",
    "        dist = distance(data,centroids)\n",
    "        \n",
    "        ## give cluster label to each point \n",
    "        cluster_label = np.argmin(dist, axis=1)\n",
    "        \n",
    "        ## calculate new centroids\n",
    "        newCentroids = np.zeros(centroids.shape)\n",
    "        for j in range(0, k):\n",
    "            if sum(cluster_label == j) == 0:\n",
    "                newCentroids[j] = centroids[j]\n",
    "            else:\n",
    "                newCentroids[j] = np.mean(data[cluster_label == j, :], axis=0)\n",
    "        \n",
    "        ## Check if it is converged\n",
    "        if np.array_equal(centroids, newCentroids):\n",
    "            print(\"Converge! after:\",iterations,\"iterations\")\n",
    "            break \n",
    "            \n",
    "        centroids = newCentroids\n",
    "        iterations += 1\n",
    "        \n",
    "    return({\"Iteration before Coverge\": iterations, \n",
    "            \"Centroids\": centroids, \n",
    "            \"Labels\": cluster_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 K-Means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kmeanspp_func.py\n"
     ]
    }
   ],
   "source": [
    "%%file kmeanspp_func.py\n",
    "import numpy as np\n",
    "from distance_func import distance\n",
    "\n",
    "def cost(dist):\n",
    "    \"\"\" Calculate the cost of data with respect to the current centroids\n",
    "    Parameters:\n",
    "       dist     distance matrix between data and current centroids   \n",
    "    Returns:    the normalized constant in the distribution \n",
    "    \"\"\"\n",
    "    return np.sum(np.min(dist,axis=1))\n",
    "\n",
    "def distribution(dist,cost):\n",
    "    \"\"\" Calculate the distribution to sample new centers\n",
    "    Parameters:\n",
    "       dist       distance matrix between data and current centroids\n",
    "       cost       the cost of data with respect to the current centroids\n",
    "    Returns:      distribution \n",
    "    \"\"\"\n",
    "    return np.min(dist, axis=1)/cost\n",
    "\n",
    "def sample_new(data,distribution,l):\n",
    "    \"\"\" Sample new centers\n",
    "    Parameters:\n",
    "       data         n*d\n",
    "       distribution n*1\n",
    "       l            the number of new centers to sample\n",
    "    Returns:        new centers                          \n",
    "    \"\"\"\n",
    "    return data[np.random.choice(range(len(distribution)),l,p=distribution),:]\n",
    "\n",
    "def KMeansPlusPlus(data, k):    \n",
    "    \"\"\" Apply the KMeans++ clustering algorithm to get the initial centroids   \n",
    "    Parameters: \n",
    "      data                        ndarrays data \n",
    "      k                           number of cluster  \n",
    "    Returns:\n",
    "      \"Centroids\"                 the complete initial centroids by KMeans++\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize the first centroid\n",
    "    centroids = data[np.random.choice(data.shape[0],1),:]\n",
    "    \n",
    "    while centroids.shape[0] < k :\n",
    "                \n",
    "        #Get the distance between data and centroids\n",
    "        dist = distance(data, centroids)\n",
    "        \n",
    "        #Calculate the cost of data with respect to the centroids\n",
    "        norm_const = cost(dist)\n",
    "        \n",
    "        #Calculate the distribution for sampling a new center\n",
    "        p = distribution(dist,norm_const)\n",
    "        \n",
    "        #Sample the new center and append it to the original ones\n",
    "        centroids = np.r_[centroids, sample_new(data,p,1)]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Kmeans Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scalablekmeanspp_func.py\n"
     ]
    }
   ],
   "source": [
    "%%file scalablekmeanspp_func.py\n",
    "import numpy as np\n",
    "from distance_func import distance\n",
    "from kmeanspp_func import cost, distribution, sample_new\n",
    "\n",
    "def get_weight(dist,centroids):\n",
    "    \"\"\" Get the weight of each centroid\n",
    "    \n",
    "    Parameters:\n",
    "    dist      matrix of distance between data and current centroids\n",
    "    centroids current centroids\n",
    "    \n",
    "    Returns weight of each centroid\n",
    "    \"\"\"\n",
    "    min_dist = np.zeros(dist.shape)\n",
    "    min_dist[range(dist.shape[0]), np.argmin(dist, axis=1)] = 1\n",
    "    count = np.array([np.count_nonzero(min_dist[:, i]) for i in range(centroids.shape[0])])\n",
    "    return count/np.sum(count)\n",
    "\n",
    "def weightedKMeans(data, k, weight, centroids, max_iter = 10000): \n",
    "    \n",
    "    \"\"\" Apply the weighted KMeans clustering algorithm\n",
    "    \n",
    "    Parameters:\n",
    "      data                        ndarrays data \n",
    "      k                           number of cluster\n",
    "      weight                      weight matrix of data\n",
    "      centroids                   initial centroids\n",
    "    \n",
    "    Returns:\n",
    "      \"Iteration before Coverge\"  time used to converge\n",
    "      \"Centroids\"                 the final centroids finded by KMeans    \n",
    "      \"Labels\"                    the cluster of each data   \n",
    "    \"\"\"\n",
    "    \n",
    "    n = data.shape[0] \n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iter:        \n",
    "        dist = distance(data, centroids) * weight[:, np.newaxis]\n",
    "        \n",
    "        ## give cluster label to each point \n",
    "        cluster_label = np.argmin(dist, axis=1)\n",
    "        \n",
    "        ## calculate new centroids\n",
    "        newCentroids = np.zeros(centroids.shape)\n",
    "        for j in range(0, k):\n",
    "            if sum(cluster_label == j) == 0:\n",
    "                newCentroids[j] = centroids[j]\n",
    "            else:\n",
    "                newCentroids[j] = np.mean(data[cluster_label == j, :], axis=0)\n",
    "        \n",
    "        ## Check if it is converged\n",
    "        if np.array_equal(centroids, newCentroids):\n",
    "            print(\"Converge\")\n",
    "            break \n",
    "        \n",
    "        centroids = newCentroids\n",
    "        iterations += 1\n",
    "        \n",
    "    return(centroids)\n",
    "\n",
    "def ScalableKMeansPlusPlus(data, k, l, weighted=False, iter=5):\n",
    "    \n",
    "    \"\"\" Apply the KMeans|| clustering algorithm   \n",
    "    Parameters:\n",
    "      data     ndarrays data \n",
    "      k        number of cluster\n",
    "      l        number of point sampled in each iteration\n",
    "      weighted if True, using weighted reclustering in the last step \n",
    "                  else, using weighted sampling in the last step.\n",
    "    \n",
    "    Returns:   the initial centroids finded by KMeans||  \n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    centroids = data[np.random.choice(range(data.shape[0]),1), :]\n",
    "    \n",
    "    \n",
    "    for i in range(iter):\n",
    "        #Get the distance between data and centroids\n",
    "        dist = distance(data, centroids)\n",
    "        \n",
    "        #Calculate the cost of data with respect to the centroids\n",
    "        norm_const = cost(dist)\n",
    "        \n",
    "        \n",
    "        #Calculate the distribution for sampling l new centers\n",
    "        p = distribution(dist,norm_const)\n",
    "        \n",
    "        #Sample the l new centers and append them to the original ones\n",
    "        centroids = np.r_[centroids, sample_new(data,p,l)]\n",
    "    \n",
    "\n",
    "    ## reduce k*l to k using KMeans++ \n",
    "    dist = distance(data, centroids)\n",
    "    weights = get_weight(dist, centroids)\n",
    "    if weighted:\n",
    "        initial = centroids[np.random.choice(range(len(weights)),k,replace=False),:]\n",
    "        centers=  weightedKMeans(centroids, k, weights, initial)\n",
    "    else: \n",
    "        centers= centroids[np.random.choice(len(weights), k, replace= False, p = weights),:]\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we implement some unit tests to ensure all the functions work.\n",
    "- For **distance** function, we test its non-negativity, symmetry and accuracy of outputs given some spacific inputs.\n",
    "- For **cost function**, we test its nongegativity.\n",
    "- For **distribution** fucntion, we test its non-negativity and whether the output sums up to 1 or not.\n",
    "- For **sample_new** function, we test the length of the sample and whether all the sample points belong to the original data sst.\n",
    "- For **get_weight** function, we test the weights are non-negative and sum up to 1.\n",
    "- For **KMeans** fucntion, we test the length of the labels and the number of different labels.\n",
    "- For **KMeansPlusPlus** and **ScalableKMeansPlusPlus**, we test the lengths equal to the inputs and the initilizations are in the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_distance.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_distance.py\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "from distance_func import distance\n",
    "\n",
    "def test_non_negativity():\n",
    "    u = np.random.normal(size=(3,4))\n",
    "    v = np.random.normal(size=(5,4))\n",
    "    assert (distance(u, v)>= 0).all()\n",
    "    \n",
    "def test_coincidence_when_zero():\n",
    "    u = np.zeros((3,4))\n",
    "    v = np.zeros((5,4))\n",
    "    assert (distance(u, v)==0).all()\n",
    "\n",
    "def test_coincidence_when_not_zero():\n",
    "    u = np.random.normal(size=(3,4))\n",
    "    v = np.random.normal(size=(5,4))\n",
    "    assert (distance(u, v)!= 0).any()\n",
    "\n",
    "def test_symmetry():\n",
    "    u = np.random.normal(size=(3,4))\n",
    "    v = np.random.normal(size=(5,4))\n",
    "    assert (distance(u, v)== distance(v, u).T).all()\n",
    "\n",
    "def test_known1():\n",
    "    u = np.array([[0,0],[1,1]])\n",
    "    v = np.array([[0,0],[1,1]])\n",
    "    dist = np.array([[0,2],[2,0]])\n",
    "    assert_almost_equal(distance(u, v), dist)\n",
    "    \n",
    "def test_known2():\n",
    "    u = np.array([[0,0,0],[1,1,1],[2,2,2]])\n",
    "    v = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "    dist = np.array([[3,12,27],[0,3,12],[3,0,3]])\n",
    "    assert_almost_equal(distance(u, v), dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_cost.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_cost.py\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "from kmeanspp_func import cost\n",
    "from distance_func import distance\n",
    "\n",
    "def test_non_negative():\n",
    "    for i in range(10):\n",
    "        data = np.random.normal(size=(5,4))\n",
    "        c = data[np.random.choice(range(4),2),]\n",
    "        dist = distance(data,c)\n",
    "        assert cost(dist) >= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_distribution.py\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "from distance_func import distance\n",
    "from kmeanspp_func import distribution,cost\n",
    "\n",
    "def test_non_negative():\n",
    "    data = np.random.normal(size=(20,4))\n",
    "    centroids = data[np.random.choice(range(4),4),]\n",
    "    dist = distance(data,centroids)\n",
    "    c = cost(dist)\n",
    "    p = distribution(dist,c)\n",
    "    assert (p>=0).all()\n",
    "    \n",
    "def test_sum_to_one():\n",
    "    data = np.random.normal(size=(20,4))\n",
    "    centroids = data[np.random.choice(range(4),4),]\n",
    "    dist = distance(data,centroids)\n",
    "    c = cost(dist)\n",
    "    p = distribution(dist,c)\n",
    "    assert_almost_equal(np.sum(p),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_sample_new.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_sample_new.py\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "from distance_func import distance\n",
    "from kmeanspp_func import cost, sample_new, distribution\n",
    "\n",
    "def test_length():\n",
    "    data = np.random.normal(size=(20,4))\n",
    "    centroids = data[np.random.choice(range(4),4),]\n",
    "    dist = distance(data,centroids)\n",
    "    c = cost(dist)\n",
    "    p = distribution(dist,c)\n",
    "    l = 5\n",
    "    c_new = sample_new(data,p,l)\n",
    "    assert len(c_new)==5\n",
    "\n",
    "def test_in_data():\n",
    "    data = np.random.normal(size=(20,4))\n",
    "    centroids = data[np.random.choice(range(4),4),]\n",
    "    dist = distance(data,centroids)\n",
    "    c = cost(dist)\n",
    "    p = distribution(dist,c)\n",
    "    l = 5\n",
    "    c_new = sample_new(data,p,l)\n",
    "    check = [i in data for i in c_new]\n",
    "    assert all(check)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_weights.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_weights.py\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "from distance_func import distance\n",
    "from scalablekmeanspp_func import get_weight\n",
    "\n",
    "def test_non_negative():\n",
    "    data = np.random.normal(size=(20,4))\n",
    "    centroids = data[np.random.choice(range(4),4),]\n",
    "    dist = distance(data,centroids)\n",
    "    w = get_weight(dist,centroids)\n",
    "    assert (w>=0).all()\n",
    "    \n",
    "def test_sum_to_one():\n",
    "    data = np.random.normal(size=(20,4))\n",
    "    centroids = data[np.random.choice(range(4),4),]\n",
    "    dist = distance(data,centroids)\n",
    "    w = get_weight(dist,centroids)\n",
    "    assert_almost_equal(np.sum(w),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_kmeans.py\n",
    "import numpy as np\n",
    "from distance_func import distance\n",
    "from kmeans_func import KMeans\n",
    "\n",
    "def test_label():\n",
    "    for i in range(10):\n",
    "        data = np.random.normal(size=(50,2))\n",
    "        k = 3\n",
    "        centroids = data[np.random.choice(range(data.shape[0]), k, replace=False),:]\n",
    "        label = KMeans(data, k, centroids)[\"Labels\"]\n",
    "        assert max(label) == k-1 and len(label)==data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_kmeanspp.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_kmeanspp.py\n",
    "import numpy as np\n",
    "from kmeanspp_func import KMeansPlusPlus\n",
    "from scalablekmeanspp_func import ScalableKMeansPlusPlus\n",
    "\n",
    "def test_length():\n",
    "    data = np.random.normal(size=(2000,2))\n",
    "    k = 3\n",
    "    l = 5\n",
    "    ini1 = KMeansPlusPlus(data, k)\n",
    "    ini2 = ScalableKMeansPlusPlus(data, k, l)\n",
    "    assert len(ini1)==k and len(ini2)==k\n",
    "\n",
    "def test_length():\n",
    "    data = np.random.normal(size=(2000,2))\n",
    "    k = 3\n",
    "    l = 5\n",
    "    ini1 = KMeansPlusPlus(data, k)\n",
    "    ini2 = ScalableKMeansPlusPlus(data, k, l)\n",
    "    check1 = [i in data for i in ini1]\n",
    "    check2 = [i in data for i in ini1]\n",
    "    assert all(check1) and all(check2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 2.7.11, pytest-2.8.5, py-1.4.31, pluggy-0.3.1\n",
      "rootdir: C:\\Users\\Raghav\\Documents\\GitHub\\Python-for-Data-Science\\STA663\\Projects\\Scalable KMeans PlusPlus 2\\Fu Wen,Xin Xu_Bahmani_Scalable K-Means++, inifile: \n",
      "collected 15 items\n",
      "\n",
      "test_cost.py .\n",
      "test_distance.py ......\n",
      "test_distribution.py ..\n",
      "test_kmeans.py .\n",
      "test_kmeanspp.py F\n",
      "test_sample_new.py ..\n",
      "test_weights.py .F\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_________________________________ test_length _________________________________\n",
      "\n",
      "    def test_length():\n",
      "        data = np.random.normal(size=(2000,2))\n",
      "        k = 3\n",
      "        l = 5\n",
      "        ini1 = KMeansPlusPlus(data, k)\n",
      ">       ini2 = ScalableKMeansPlusPlus(data, k, l)\n",
      "\n",
      "test_kmeanspp.py:18: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "scalablekmeanspp_func.py:101: in ScalableKMeansPlusPlus\n",
      "    centers= centroids[np.random.choice(len(weights), k, replace= False, p = weights),:]\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      ">   ???\n",
      "E   ValueError: probabilities do not sum to 1\n",
      "\n",
      "mtrand.pyx:1104: ValueError\n",
      "_______________________________ test_sum_to_one _______________________________\n",
      "\n",
      "    def test_sum_to_one():\n",
      "        data = np.random.normal(size=(20,4))\n",
      "        centroids = data[np.random.choice(range(4),4),]\n",
      "        dist = distance(data,centroids)\n",
      "        w = get_weight(dist,centroids)\n",
      ">       assert_almost_equal(np.sum(w),1)\n",
      "E       AssertionError: \n",
      "E       Arrays are not almost equal to 7 decimals\n",
      "E        ACTUAL: 0\n",
      "E        DESIRED: 1\n",
      "\n",
      "test_weights.py:19: AssertionError\n",
      "===================== 2 failed, 13 passed in 0.99 seconds =====================\n"
     ]
    }
   ],
   "source": [
    "! py.test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we simply use Cython to speed up our multiple fucntions. Since we've already applies broadcasting and vectorize those functions as possible as we can in the previous part, the speeding up performance is not that obvious.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DistutilsPlatformError",
     "evalue": "Unable to find vcvarsall.bat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDistutilsPlatformError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b234a2d92276>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cython'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'import numpy as np\\ncimport numpy as np\\n\\ndef distance_cy(data, centroids):\\n    \"\"\" Calculate the distance from each data point to each center\\n    Parameters:\\n       data   n*d\\n       center k*d\\n    \\n    Returns:\\n       distence n*k \\n    \"\"\"\\n    ## calculate distence between each point to the centroids\\n    dist = np.sum((data[:, np.newaxis, :] - centroids)**2, axis=2)\\n    return dist\\n\\ndef KMeans_cy(data, k, centroids, max_iter = 10000): \\n    \\n    \"\"\" Apply the KMeans clustering algorithm\\n    \\n    Parameters:\\n      data                        ndarrays data \\n      k                           number of cluster\\n      centroids                   initial centroids\\n    \\n    Returns:\\n      \"Iteration before Coverge\"  time used to converge\\n      \"Centroids\"                 the final centroids finded by KMeans    \\n      \"Labels\"                    the cluster of each data   \\n    \"\"\"\\n    \\n    n = data.shape[0] \\n    iterations = 0\\n    \\n    while iterations < max_iter:        \\n        dist = distance_cy(data,centroids)\\n        \\n        ## give cluster label to each point \\n        cluster_label = np.argmin(dist, axis=1)\\n        \\n        ## calculate new centroids\\n        newCentroids = np.zeros(centroids.shape)\\n        for j in range(0, k):\\n            if sum(cluster_label == j) == 0:\\n                newCentroids[j] = centroids[j]\\n            else:\\n                newCentroids[j] = np.mean(data[cluster_label == j, :], axis=0)\\n        \\n        ## Check if it is converged\\n        if np.array_equal(centroids, newCentroids):\\n            print(\"Converge\")\\n            break \\n            \\n        centroids = newCentroids\\n        iterations += 1\\n        \\n    return({\"Iteration before Coverge\": iterations, \\n            \"Centroids\": centroids, \\n            \"Labels\": cluster_label})\\n\\ndef cost_cy(dist):\\n    \"\"\" Calculate the cost of data with respect to the current centroids\\n    Parameters:\\n       dist     distance matrix between data and current centroids\\n    \\n    Returns:    the normalized constant in the distribution \\n    \"\"\"\\n    return np.sum(np.min(dist,axis=1))\\n\\ndef distribution_cy(dist,cost):\\n    \"\"\" Calculate the distribution to sample new centers\\n    Parameters:\\n       dist       distance matrix between data and current centroids\\n       cost       the cost of data with respect to the current centroids\\n    Returns:      distribution \\n    \"\"\"\\n    return np.min(dist, axis=1)/cost\\n\\ndef sample_new_cy(data,distribution,l):\\n    \"\"\" Sample new centers\\n    \\n    Parameters:\\n       data         n*d\\n       distribution n*1\\n       l            the number of new centers to sample\\n    Returns:        new centers                          \\n    \"\"\"\\n    return data[np.random.choice(range(len(distribution)),l,p=distribution),:]\\n\\ndef KMeansPlusPlus_cy(data, k):    \\n    \"\"\" Apply the KMeans++ clustering algorithm to get the initial centroids   \\n    Parameters: \\n      data                        ndarrays data \\n      k                           number of cluster\\n    \\n    Returns:\\n      \"Centroids\"                 the complete initial centroids by KMeans++\\n      \\n    \"\"\"\\n    \\n    #Initialize the first centroid\\n    centroids = data[np.random.choice(data.shape[0],1),:]\\n    \\n    while centroids.shape[0] < k :\\n                \\n        #Get the distance between data and centroids\\n        dist = distance_cy(data, centroids)\\n        \\n        #Calculate the cost of data with respect to the centroids\\n        norm_const = cost_cy(dist)\\n        \\n        #Calculate the distribution for sampling a new center\\n        p = distribution_cy(dist,norm_const)\\n        \\n        #Sample the new center and append it to the original ones\\n        centroids = np.r_[centroids, sample_new_cy(data,p,1)]\\n    \\n    return centroids\\n\\ndef get_weight_cy(dist,centroids):\\n    \"\"\" Get the weight of each centroid\\n    \\n    Parameters:\\n    dist      matrix of distance between data and current centroids\\n    centroids current centroids\\n    \\n    Returns weight of each centroid\\n    \"\"\"\\n    min_dist = np.zeros(dist.shape)\\n    min_dist[range(dist.shape[0]), np.argmin(dist, axis=1)] = 1\\n    count = np.array([np.count_nonzero(min_dist[:, i]) for i in range(centroids.shape[0])])\\n    return count/np.sum(count)\\n\\ndef weightedKMeans(data, k, weight, centroids, max_iter = 10000): \\n    \\n    \"\"\" Apply the weighted KMeans clustering algorithm\\n    \\n    Parameters:\\n      data                        ndarrays data \\n      k                           number of cluster\\n      weight                      weight matrix of data\\n      centroids                   initial centroids\\n    \\n    Returns:\\n      \"Iteration before Coverge\"  time used to converge\\n      \"Centroids\"                 the final centroids finded by KMeans    \\n      \"Labels\"                    the cluster of each data   \\n    \"\"\"\\n    \\n    n = data.shape[0] \\n    iterations = 0\\n    \\n    while iterations < max_iter:        \\n        dist = distance_cy(data, centroids) * weight[:, np.newaxis]\\n        \\n        ## give cluster label to each point \\n        cluster_label = np.argmin(dist, axis=1)\\n        \\n        ## calculate new centroids\\n        newCentroids = np.zeros(centroids.shape)\\n        for j in range(0, k):\\n            if sum(cluster_label == j) == 0:\\n                newCentroids[j] = centroids[j]\\n            else:\\n                newCentroids[j] = np.mean(data[cluster_label == j, :], axis=0)\\n        \\n        ## Check if it is converged\\n        if np.array_equal(centroids, newCentroids):\\n            print(\"Converge\")\\n            break \\n        \\n        centroids = newCentroids\\n        iterations += 1\\n        \\n    return(centroids)\\n\\n\\ndef ScalableKMeansPlusPlus_cy(data, k, l, weighted=False, iter=5):\\n    \\n    \"\"\" Apply the KMeans|| clustering algorithm\\n    \\n    Parameters:\\n      data     ndarrays data \\n      k        number of cluster\\n      l        number of point sampled in each iteration\\n      weighted if True, using weighted reclustering in the last step \\n                  else, using weighted sampling in the last step.\\n    \\n    Returns:   the final centroids finded by KMeans||  \\n      \\n    \"\"\"\\n    \\n    centroids = data[np.random.choice(range(data.shape[0]),1), :]\\n    \\n    \\n    for i in range(iter):\\n        #Get the distance between data and centroids\\n        dist = distance_cy(data, centroids)\\n        \\n        #Calculate the cost of data with respect to the centroids\\n        norm_const = cost_cy(dist)\\n        \\n        #Calculate the distribution for sampling l new centers\\n        p = distribution_cy(dist,norm_const)\\n        \\n        #Sample the l new centers and append them to the original ones\\n        centroids = np.r_[centroids, sample_new_cy(data,p,l)]\\n    \\n\\n    ## reduce k*l to k using KMeans++ \\n    dist = distance_cy(data, centroids)\\n    weights = get_weight_cy(dist,centroids)\\n    if weighted:\\n        initial = centroids[np.random.choice(range(len(weights)),k,replace=False),:]\\n        centers=  weightedKMeans(centroids, k, weights, initial)\\n    else: \\n        centers= centroids[np.random.choice(len(weights), k, replace= False, p = weights),:]\\n    return centers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-127>\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\Cython\\Build\\IpythonMagic.py\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyx_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_lib\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlib_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_code_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;31m# Now actually compile and link everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_extensions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_serial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36m_build_extensions_serial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_build_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extension\u001b[1;34m(self, ext)\u001b[0m\n\u001b[0;32m    530\u001b[0m                                          \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m                                          \u001b[0mextra_postargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m                                          depends=ext.depends)\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;31m# XXX outdated variable, kept here in case third-part code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         compile_info = self._setup_compile(output_dir, macros, include_dirs,\n\u001b[0;32m    319\u001b[0m                                            sources, depends, extra_postargs)\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, plat_name)\u001b[0m\n\u001b[0;32m    208\u001b[0m             )\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mvc_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_vc_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvc_env\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             raise DistutilsPlatformError(\"Unable to find a compatible \"\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36m_get_vc_env\u001b[1;34m(plat_spec)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mvcvarsall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvcruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_find_vcvarsall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvcvarsall\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mDistutilsPlatformError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to find vcvarsall.bat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDistutilsPlatformError\u001b[0m: Unable to find vcvarsall.bat"
     ]
    }
   ],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "def distance_cy(data, centroids):\n",
    "    \"\"\" Calculate the distance from each data point to each center\n",
    "    Parameters:\n",
    "       data   n*d\n",
    "       center k*d\n",
    "    \n",
    "    Returns:\n",
    "       distence n*k \n",
    "    \"\"\"\n",
    "    ## calculate distence between each point to the centroids\n",
    "    dist = np.sum((data[:, np.newaxis, :] - centroids)**2, axis=2)\n",
    "    return dist\n",
    "\n",
    "def KMeans_cy(data, k, centroids, max_iter = 10000): \n",
    "    \n",
    "    \"\"\" Apply the KMeans clustering algorithm\n",
    "    \n",
    "    Parameters:\n",
    "      data                        ndarrays data \n",
    "      k                           number of cluster\n",
    "      centroids                   initial centroids\n",
    "    \n",
    "    Returns:\n",
    "      \"Iteration before Coverge\"  time used to converge\n",
    "      \"Centroids\"                 the final centroids finded by KMeans    \n",
    "      \"Labels\"                    the cluster of each data   \n",
    "    \"\"\"\n",
    "    \n",
    "    n = data.shape[0] \n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iter:        \n",
    "        dist = distance_cy(data,centroids)\n",
    "        \n",
    "        ## give cluster label to each point \n",
    "        cluster_label = np.argmin(dist, axis=1)\n",
    "        \n",
    "        ## calculate new centroids\n",
    "        newCentroids = np.zeros(centroids.shape)\n",
    "        for j in range(0, k):\n",
    "            if sum(cluster_label == j) == 0:\n",
    "                newCentroids[j] = centroids[j]\n",
    "            else:\n",
    "                newCentroids[j] = np.mean(data[cluster_label == j, :], axis=0)\n",
    "        \n",
    "        ## Check if it is converged\n",
    "        if np.array_equal(centroids, newCentroids):\n",
    "            print(\"Converge\")\n",
    "            break \n",
    "            \n",
    "        centroids = newCentroids\n",
    "        iterations += 1\n",
    "        \n",
    "    return({\"Iteration before Coverge\": iterations, \n",
    "            \"Centroids\": centroids, \n",
    "            \"Labels\": cluster_label})\n",
    "\n",
    "def cost_cy(dist):\n",
    "    \"\"\" Calculate the cost of data with respect to the current centroids\n",
    "    Parameters:\n",
    "       dist     distance matrix between data and current centroids\n",
    "    \n",
    "    Returns:    the normalized constant in the distribution \n",
    "    \"\"\"\n",
    "    return np.sum(np.min(dist,axis=1))\n",
    "\n",
    "def distribution_cy(dist,cost):\n",
    "    \"\"\" Calculate the distribution to sample new centers\n",
    "    Parameters:\n",
    "       dist       distance matrix between data and current centroids\n",
    "       cost       the cost of data with respect to the current centroids\n",
    "    Returns:      distribution \n",
    "    \"\"\"\n",
    "    return np.min(dist, axis=1)/cost\n",
    "\n",
    "def sample_new_cy(data,distribution,l):\n",
    "    \"\"\" Sample new centers\n",
    "    \n",
    "    Parameters:\n",
    "       data         n*d\n",
    "       distribution n*1\n",
    "       l            the number of new centers to sample\n",
    "    Returns:        new centers                          \n",
    "    \"\"\"\n",
    "    return data[np.random.choice(range(len(distribution)),l,p=distribution),:]\n",
    "\n",
    "def KMeansPlusPlus_cy(data, k):    \n",
    "    \"\"\" Apply the KMeans++ clustering algorithm to get the initial centroids   \n",
    "    Parameters: \n",
    "      data                        ndarrays data \n",
    "      k                           number of cluster\n",
    "    \n",
    "    Returns:\n",
    "      \"Centroids\"                 the complete initial centroids by KMeans++\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize the first centroid\n",
    "    centroids = data[np.random.choice(data.shape[0],1),:]\n",
    "    \n",
    "    while centroids.shape[0] < k :\n",
    "                \n",
    "        #Get the distance between data and centroids\n",
    "        dist = distance_cy(data, centroids)\n",
    "        \n",
    "        #Calculate the cost of data with respect to the centroids\n",
    "        norm_const = cost_cy(dist)\n",
    "        \n",
    "        #Calculate the distribution for sampling a new center\n",
    "        p = distribution_cy(dist,norm_const)\n",
    "        \n",
    "        #Sample the new center and append it to the original ones\n",
    "        centroids = np.r_[centroids, sample_new_cy(data,p,1)]\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "def get_weight_cy(dist,centroids):\n",
    "    \"\"\" Get the weight of each centroid\n",
    "    \n",
    "    Parameters:\n",
    "    dist      matrix of distance between data and current centroids\n",
    "    centroids current centroids\n",
    "    \n",
    "    Returns weight of each centroid\n",
    "    \"\"\"\n",
    "    min_dist = np.zeros(dist.shape)\n",
    "    min_dist[range(dist.shape[0]), np.argmin(dist, axis=1)] = 1\n",
    "    count = np.array([np.count_nonzero(min_dist[:, i]) for i in range(centroids.shape[0])])\n",
    "    return count/np.sum(count)\n",
    "\n",
    "def weightedKMeans(data, k, weight, centroids, max_iter = 10000): \n",
    "    \n",
    "    \"\"\" Apply the weighted KMeans clustering algorithm\n",
    "    \n",
    "    Parameters:\n",
    "      data                        ndarrays data \n",
    "      k                           number of cluster\n",
    "      weight                      weight matrix of data\n",
    "      centroids                   initial centroids\n",
    "    \n",
    "    Returns:\n",
    "      \"Iteration before Coverge\"  time used to converge\n",
    "      \"Centroids\"                 the final centroids finded by KMeans    \n",
    "      \"Labels\"                    the cluster of each data   \n",
    "    \"\"\"\n",
    "    \n",
    "    n = data.shape[0] \n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iter:        \n",
    "        dist = distance_cy(data, centroids) * weight[:, np.newaxis]\n",
    "        \n",
    "        ## give cluster label to each point \n",
    "        cluster_label = np.argmin(dist, axis=1)\n",
    "        \n",
    "        ## calculate new centroids\n",
    "        newCentroids = np.zeros(centroids.shape)\n",
    "        for j in range(0, k):\n",
    "            if sum(cluster_label == j) == 0:\n",
    "                newCentroids[j] = centroids[j]\n",
    "            else:\n",
    "                newCentroids[j] = np.mean(data[cluster_label == j, :], axis=0)\n",
    "        \n",
    "        ## Check if it is converged\n",
    "        if np.array_equal(centroids, newCentroids):\n",
    "            print(\"Converge\")\n",
    "            break \n",
    "        \n",
    "        centroids = newCentroids\n",
    "        iterations += 1\n",
    "        \n",
    "    return(centroids)\n",
    "\n",
    "\n",
    "def ScalableKMeansPlusPlus_cy(data, k, l, weighted=False, iter=5):\n",
    "    \n",
    "    \"\"\" Apply the KMeans|| clustering algorithm\n",
    "    \n",
    "    Parameters:\n",
    "      data     ndarrays data \n",
    "      k        number of cluster\n",
    "      l        number of point sampled in each iteration\n",
    "      weighted if True, using weighted reclustering in the last step \n",
    "                  else, using weighted sampling in the last step.\n",
    "    \n",
    "    Returns:   the final centroids finded by KMeans||  \n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    centroids = data[np.random.choice(range(data.shape[0]),1), :]\n",
    "    \n",
    "    \n",
    "    for i in range(iter):\n",
    "        #Get the distance between data and centroids\n",
    "        dist = distance_cy(data, centroids)\n",
    "        \n",
    "        #Calculate the cost of data with respect to the centroids\n",
    "        norm_const = cost_cy(dist)\n",
    "        \n",
    "        #Calculate the distribution for sampling l new centers\n",
    "        p = distribution_cy(dist,norm_const)\n",
    "        \n",
    "        #Sample the l new centers and append them to the original ones\n",
    "        centroids = np.r_[centroids, sample_new_cy(data,p,l)]\n",
    "    \n",
    "\n",
    "    ## reduce k*l to k using KMeans++ \n",
    "    dist = distance_cy(data, centroids)\n",
    "    weights = get_weight_cy(dist,centroids)\n",
    "    if weighted:\n",
    "        initial = centroids[np.random.choice(range(len(weights)),k,replace=False),:]\n",
    "        centers=  weightedKMeans(centroids, k, weights, initial)\n",
    "    else: \n",
    "        centers= centroids[np.random.choice(len(weights), k, replace= False, p = weights),:]\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. High performance computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use `multiprocessing`, we speed up `cost` function and `sample_new` function with all cpu cores. So, we parallelly calculate the distance from each point to its nearest center and its probability distribution for next sample in `cost` function. In `sample_new` function, we sample $l$ new centroids parallelly, which is the most meaningful part of `KMeans ||` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from distance_func import distance\n",
    "\n",
    "def min_distance(d, centroids):\n",
    "    \n",
    "    \"\"\" Calculate the minimum distance from point d \n",
    "        to its nearest center in centroids.\"\"\"\n",
    "    dist = np.min(np.sum((centroids - d)**2, axis=1))\n",
    "    return dist\n",
    "\n",
    "\n",
    "# speed up by multiprocessing\n",
    "def cost_p(data, centroids): \n",
    "    \n",
    "    \"\"\" Calculate the cost of data with respect to \n",
    "    the current centroids and the new probability \n",
    "    distribution of each point for next sample\"\"\"    \n",
    "\n",
    "    with Pool(processes = cpu_count()) as pool:\n",
    "        partial_dist = partial(min_distance, centroids = centroids)\n",
    "        min_dist = pool.map(partial_dist, data)\n",
    "        cost = np.sum(min_dist)\n",
    "        p = min_dist/cost\n",
    "    return cost,p\n",
    "\n",
    "\n",
    "def randomSample(x, a, p):\n",
    "    np.random.seed()\n",
    "    return np.random.choice(a = a, size = x , p =p)\n",
    "\n",
    "\n",
    "## speed up with multiprocessing\n",
    "def sample_new_p(data, distribution, l):\n",
    "    \n",
    "    \"\"\" Sample new centers\"\"\"  \n",
    "    \n",
    "    with Pool(processes = cpu_count()) as pool:\n",
    "        partial_rc = partial(randomSample, a = len(distribution), p=distribution)\n",
    "        index = pool.map(partial_rc,[1]*l)\n",
    "    return np.squeeze(data[index,:],axis=(1,))\n",
    "\n",
    "\n",
    "def min_distance_index_p(d, centroids):\n",
    "    \n",
    "    \"\"\" Return the index of the minimum distance from point d \n",
    "        to its nearest center in centroids.\"\"\"\n",
    "    \n",
    "    minInd = np.argmin(np.sum((centroids - d)**2, axis=1))\n",
    "    return minInd \n",
    "\n",
    "## speed up with multiprocessing\n",
    "def get_weight_p(data, centroids):\n",
    "    \n",
    "    \"\"\" Return weight of all centroids \"\"\"\n",
    "\n",
    "    with Pool(processes = cpu_count()) as pool:\n",
    "        partial_minInd = partial(min_distance_index_p, centroids = centroids )\n",
    "        min_index = pool.map(partial_minInd, data)\n",
    "        count = np.array([np.sum(np.array(min_index) == i) for i in range(centroids.shape[0])])\n",
    "    return count/np.sum(count)\n",
    "\n",
    "def weightedKMeans(data, k, weight, centroids, max_iter = 10000): \n",
    "    \n",
    "    \"\"\" Apply the weighted KMeans clustering algorithm\n",
    "    \n",
    "    Parameters:\n",
    "      data                        ndarrays data \n",
    "      k                           number of cluster\n",
    "      weight                      weight matrix of data\n",
    "      centroids                   initial centroids\n",
    "    \n",
    "    Returns:\n",
    "      \"Iteration before Coverge\"  time used to converge\n",
    "      \"Centroids\"                 the final centroids finded by KMeans    \n",
    "      \"Labels\"                    the cluster of each data   \n",
    "    \"\"\"\n",
    "    \n",
    "    n = data.shape[0] \n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iter:        \n",
    "        dist = distance(data, centroids) * weight[:, np.newaxis]\n",
    "        \n",
    "        ## give cluster label to each point \n",
    "        cluster_label = np.argmin(dist, axis=1)\n",
    "        \n",
    "        ## calculate new centroids\n",
    "        newCentroids = np.zeros(centroids.shape)\n",
    "        for j in range(0, k):\n",
    "            if sum(cluster_label == j) == 0:\n",
    "                newCentroids[j] = centroids[j]\n",
    "            else:\n",
    "                newCentroids[j] = np.mean(data[cluster_label == j, :], axis=0)\n",
    "        \n",
    "        ## Check if it is converged\n",
    "        if np.array_equal(centroids, newCentroids):\n",
    "            print(\"Converge\")\n",
    "            break \n",
    "        \n",
    "        centroids = newCentroids\n",
    "        iterations += 1\n",
    "        \n",
    "    return(centroids)\n",
    "\n",
    "def ScalableKMeansPlusPlus_p(data, k, l,weighted = False, iter=5):\n",
    "    \n",
    "    \"\"\" Apply the KMeans|| clustering algorithm\"\"\"\n",
    "    \n",
    "    centroids = data[np.random.choice(range(data.shape[0]),1), :]    \n",
    "    \n",
    "    for i in range(iter):   \n",
    "        \n",
    "        # Calculate the cost and new distribution\n",
    "        norm_const = cost_p(data, centroids)[0]\n",
    "        p = cost_p(data, centroids)[1] \n",
    "        \n",
    "        # Sample the several(l) new centers and append them to the original ones\n",
    "        centroids = np.r_[centroids, sample_new_p(data, p, l)]\n",
    "        \n",
    "    ## reduce k*l to k using KMeans++ \n",
    "    weights = get_weight_p(data,centroids)\n",
    "    if weighted:\n",
    "        initial = centroids[np.random.choice(range(len(weights)),k,replace=False),:]\n",
    "        centers=  weightedKMeans(centroids, k, weights, initial)\n",
    "    else: \n",
    "        centers= centroids[np.random.choice(len(weights), k, replace= False, p = weights),:]\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Application and comparison\n",
    "In this part, we test the two statements as in Bahmani's Paper[2]:\n",
    "\n",
    "- `k-means||` obtains as a solution whose clustering cost is on par with `k-means++` and hence is expexcted to be much better than random.\n",
    "\n",
    "- `k-means||` runs in a fewer number of rounds when compared to `k-means++`, which translates into a faster running time especially in the parallel implementation.\n",
    "\n",
    "based on a simulated dataset and a real-world dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Simulate Data \n",
    "\n",
    "To generate the dataset GAUSSMIXTURE, we sampled k=5 centers from a 15-dimensional spherical Gaussian distribution with mean at the origin and variance $R\\in \\{1,10,100 \\}$ and then added points from Gaussian distributions with unit variance around each center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simulate data\n",
    "np.random.seed(1234)\n",
    "k = 20\n",
    "n = 10000\n",
    "d = 15\n",
    "\n",
    "## simulate k centers from 15-dimensional spherical Gaussian distribution \n",
    "mean = np.hstack(np.zeros((d,1)))\n",
    "cov = np.diag(np.array([1,10,100]*5))\n",
    "centers = np.random.multivariate_normal(mean, cov, k)\n",
    "\n",
    "## Simulate n data\n",
    "for i in range(k):\n",
    "    mean = centers[i]\n",
    "    if i == 0:\n",
    "        data = np.random.multivariate_normal(mean, np.diag(np.ones(d)), int(n/k+n%k))\n",
    "        trueLabels = np.repeat(i,int(n/k+n%k))\n",
    "    else:\n",
    "        data = np.append(data, np.random.multivariate_normal(mean, np.diag(np.ones(d)) , int(n/k)), axis = 0) \n",
    "        trueLabels = np.append(trueLabels,np.repeat(i,int(n/k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the first 4 dimension dimension of data as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "data_v = DataFrame(data[:,0:3]) \n",
    "data_v['trueLabels'] = trueLabels\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.3)\n",
    "sns.pairplot(data_v, hue=\"trueLabels\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement K-means with Ramdom Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kmeans_func import KMeans\n",
    "centroids_initial = data[np.random.choice(range(data.shape[0]), k, replace=False),:]\n",
    "output_k = KMeans(data, k, centroids_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot the first two dimensions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, k)]\n",
    "\n",
    "centroids1 =output_k[\"Centroids\"]\n",
    "labels1 = output_k[\"Labels\"]\n",
    "\n",
    "for i,color in enumerate(colors,start =1):\n",
    "    plt.scatter(data[labels1==i, :][:,0], data[labels1==i, :][:,1], color=color)\n",
    "\n",
    "for j in range(k):\n",
    "    plt.scatter(centroids1[j,0],centroids1[j,1],color = 'w',marker='x')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement KMeans with KMeans++ Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kmeans_func import KMeans\n",
    "from kmeanspp_func import KMeansPlusPlus\n",
    "\n",
    "centroids_initial = KMeansPlusPlus(data, 20)\n",
    "output_kpp = KMeans(data, k, centroids_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, k)]\n",
    "\n",
    "centroids1 =output_kpp[\"Centroids\"]\n",
    "labels1 = output_kpp[\"Labels\"]\n",
    "\n",
    "for i,color in enumerate(colors,start =1):\n",
    "    plt.scatter(data[labels1==i, :][:,0], data[labels1==i, :][:,1], color=color)\n",
    "\n",
    "for j in range(k):\n",
    "    plt.scatter(centroids1[j,0],centroids1[j,1],color = 'w',marker='x') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement KMeans with Scalable KMeans++ Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kmeans_func import KMeans\n",
    "from scalablekmeanspp_func import ScalableKMeansPlusPlus\n",
    "l = 10\n",
    "centroids_initial = ScalableKMeansPlusPlus(data, 20, l)\n",
    "output_spp = KMeans(data, k, centroids_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, k)]\n",
    "\n",
    "centroids1 =output_spp[\"Centroids\"]\n",
    "labels1 = output_spp[\"Labels\"]\n",
    "\n",
    "for i,color in enumerate(colors,start =1):\n",
    "    plt.scatter(data[labels1==i, :][:,0], data[labels1==i, :][:,1], color=color)\n",
    "\n",
    "for j in range(k):\n",
    "    plt.scatter(centroids1[j,0],centroids1[j,1],color = 'w',marker='x')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Misclassification Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MisClassRate(trueLabels, predict):\n",
    "    \"\"\"\n",
    "    Calculate the misclassification rate of the algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    trueLabels   n*1 true labels of each observation  \n",
    "    predict      n*1 predict labels of each observation \n",
    "    \n",
    "    Returns:     misclassification rate \n",
    "    \"\"\"\n",
    "    \n",
    "    df = DataFrame({'True':trueLabels, 'Predict':predict['Labels'],'V':1})\n",
    "    table = pd.pivot_table(df, values ='V', index = ['True'], columns=['Predict'], aggfunc=np.sum).fillna(0)\n",
    "    misRate = 1-sum(table.max(axis=1))/n\n",
    "    return misRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Misclassification Rate:\")\n",
    "print(\"Random:\", MisClassRate(trueLabels, output_k)) # Random \n",
    "print(\"KMeans++:\",MisClassRate(trueLabels, output_kpp)) # KMeans++\n",
    "print(\"Scalable KMeans++:\", MisClassRate(trueLabels, output_spp)) # Scalable KMeans++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that with the initial centroids using `KMeans++` or `scalable KMeans++` algorithm, the misclassification rate is much lower than using `Random` initial centroids, which consistent with the first statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Clustering Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from distance_func import distance\n",
    "from kmeanspp_func import cost\n",
    "\n",
    "def clusterCost(data,predict):\n",
    "    dist = distance(data,predict[\"Centroids\"])\n",
    "    return cost(dist)/(10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Clustering Cost:\")\n",
    "print(\"Random:\", clusterCost(data, output_k)) # Random \n",
    "print(\"KMeans++:\",clusterCost(data, output_kpp)) # KMeans++\n",
    "print(\"Scalable KMeans++:\", clusterCost(data, output_spp)) # Scalable KMeans++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering cost of the final results using `KMeans ++` and `Scalable KMeans++` are similar and much better than the clustering cost of random initial centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge! after: 21 iterations\n",
      "CPU times: user 7.26 s, sys: 514 ms, total: 7.78 s\n",
      "Wall time: 8.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = KMeansPlusPlus(data, k)  # KMeans++ \n",
    "b = KMeans(data,k,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge! after: 35 iterations\n",
      "CPU times: user 11.4 s, sys: 587 ms, total: 12 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = ScalableKMeansPlusPlus(data, k, l) # Scalable KMeans++\n",
    "b = KMeans(data,k,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge\n",
      "CPU times: user 10 s, sys: 455 ms, total: 10.5 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = ScalableKMeansPlusPlus_cy(data, k, l) # Scalable KMeans++ with cython \n",
    "b = KMeans_cy(data,k,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__exit__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d16ecb776e74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'a = ScalableKMeansPlusPlus_p(data, k, l) # Scalable KMeans++ with parallel\\nb = KMeans_cy(data,k,a)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda2\\lib\\site-packages\\IPython\\core\\magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda2\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-3a5e89654339>\u001b[0m in \u001b[0;36mScalableKMeansPlusPlus_p\u001b[1;34m(data, k, l, weighted, iter)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# Calculate the cost and new distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mnorm_const\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-3a5e89654339>\u001b[0m in \u001b[0;36mcost_p\u001b[1;34m(data, centroids)\u001b[0m\n\u001b[0;32m     21\u001b[0m     distribution of each point for next sample\"\"\"    \n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mpartial_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_distance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mmin_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: __exit__"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = ScalableKMeansPlusPlus_p(data, k, l) # Scalable KMeans++ with parallel\n",
    "b = KMeans_cy(data,k,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime of `KMeans++` is slow, since it sample $k$ initial centroids one by one. The runtime of `Scalable KMeans++` without speeding up is even slower than `KMeans++`, since it sample more than $k$ centroids and then clustering them again. After speeding up with `Cython` and `multiprocessing`, the `Scalable KMeans` works better in most case (not so stable). In addition, we use the `Scalable KMeans++` in Spark as follwing, it works much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 121 ms, sys: 17.9 ms, total: 139 ms\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pdata =sc.parallelize(data)\n",
    "data_model = KMeans.train(pdata, k, maxIterations=1000, runs=10, initializationMode=\"kmeans||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Real-World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"spambase.data\",\"r\")\n",
    "df = pd.read_table('spambase.data', sep=',', names=range(58))\n",
    "df = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge\n",
      "Converge\n"
     ]
    }
   ],
   "source": [
    "from kmeans_func import KMeans\n",
    "from kmeanspp_func import KMeansPlusPlus\n",
    "from scalablekmeanspp_func import ScalableKMeansPlusPlus\n",
    "\n",
    "k = 20\n",
    "\n",
    "# random\n",
    "Rcentroids_initial = df[np.random.choice(range(df.shape[0]), k, replace=False),:]\n",
    "Routput_k = KMeans_cy(df, k, Rcentroids_initial)\n",
    "\n",
    "# KMeans++\n",
    "Rcentroids_initial_kpp = KMeansPlusPlus_cy(df, k)\n",
    "Routput_kpp = KMeans_cy(df, k, Rcentroids_initial_kpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge\n",
      "Converge\n",
      "Converge\n",
      "Converge\n"
     ]
    }
   ],
   "source": [
    "# Scalable KMeans++ with l=2/k\n",
    "Rcentroids_initial_spp = ScalableKMeansPlusPlus(df, k, k/2,weighted = True)\n",
    "Routput_spp = KMeans_cy(df, k, Rcentroids_initial_spp)\n",
    "\n",
    "# Scalable KMeans++ with l=2*k\n",
    "Rcentroids_initial_spp2k = ScalableKMeansPlusPlus(df, k, k*2,weighted = True)\n",
    "Routput_spp2k = KMeans_cy(df, k, Rcentroids_initial_spp2k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Cost:\n",
      "Random: 15293.8932604\n",
      "KMeans++: 2344.39493059\n",
      "Scalable KMeans++(l=2/k): 5766.82861831\n",
      "Scalable KMeans++(l=2k): 8717.02948687\n"
     ]
    }
   ],
   "source": [
    "print(\"Clustering Cost:\")\n",
    "print(\"Random:\", clusterCost(df, Routput_k)) # Random \n",
    "print(\"KMeans++:\",clusterCost(df, Routput_kpp)) # KMeans++\n",
    "print(\"Scalable KMeans++(l=2/k):\", clusterCost(df, Routput_spp)) # Scalable KMeans++\n",
    "print(\"Scalable KMeans++(l=2k):\", clusterCost(df, Routput_spp2k)) # Scalable KMeans++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the clustering cost above, we could see that the `KMeans++` and `Scalable KMeans++` works much better than random initial centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge! after: 23 iterations\n",
      "CPU times: user 4.38 s, sys: 835 ms, total: 5.22 s\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 20\n",
    "l = 10\n",
    "a = KMeansPlusPlus(df, k)  # KMeans++ \n",
    "b = KMeans(df, k, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge! after: 55 iterations\n",
      "CPU times: user 9.38 s, sys: 1.17 s, total: 10.5 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = ScalableKMeansPlusPlus(df, k, l) # Scalable KMeans++\n",
    "b = KMeans(df,k,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge\n",
      "CPU times: user 16.2 s, sys: 1.58 s, total: 17.8 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = ScalableKMeansPlusPlus_cy(df, k, l) # Scalable KMeans++ with cython \n",
    "b = KMeans_cy(df,k,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge\n",
      "CPU times: user 24.1 s, sys: 2.52 s, total: 26.7 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = ScalableKMeansPlusPlus_p(df, k, l) # Scalable KMeans++ with parallel\n",
    "b = KMeans_cy(df,k,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 121 ms, sys: 40.3 ms, total: 161 ms\n",
      "Wall time: 5.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "pdf = sc.parallelize(df)\n",
    "df_model = KMeans.train(pdf, k, maxIterations=1000, runs=10, initializationMode=\"kmeans||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Coclusions\n",
    "\n",
    "From the implementation on the simulated data and the real-world dataset, the `k-means||` and `k-means++` find a better initial centroids than random in most cases, which leads to a better final clustering performance. Also, `k-means||` runs faster than `k-means++`, since it runs a fewer number of rounds and been speed up in the parallel implementation.\n",
    "\n",
    "However, the clustering cost of `k-means||` is not that stable. We think it might be caused by the problem in the last step. We firstly tried reclustering the multiple weighted centroids in $C$ into $k$ clusters. Since the initial center for the reclustering is picked randomly, it will have the same problem as using random initialization. Then, referring to some online resources[4], we tried sampling $k$ final centroids from the weighted centroids in $C$ in the last step. But we think it might also get several centroids in one large cluster and lead to an unstable result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] Wu, Xindong, et al. \"Top 10 algorithms in data mining.\" Knowledge and information systems 14.1 (2008): 1-37.\n",
    "\n",
    "[2] Bahmani, Bahman, et al. \"Scalable k-means++.\" Proceedings of the VLDB Endowment 5.7 (2012): 622-633\n",
    "\n",
    "[3] Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "[4] http://stats.stackexchange.com/questions/135656/k-means-a-k-a-scalable-k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
